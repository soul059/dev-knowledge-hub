# Chapter 5: Incident Response

[â† Previous: On-Call Management](04-on-call-management.md) | [Back to README](../README.md) | [Next: Incident Management â†’](06-incident-management.md)

---

## Overview

Incident response is the structured process of detecting, triaging, communicating, and resolving production incidents. A well-defined incident response process reduces downtime, minimizes customer impact, and prevents panic.

---

## 5.1 Incident Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INCIDENT LIFECYCLE                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ DETECT  â”‚â”€â”€â–ºâ”‚ TRIAGE  â”‚â”€â”€â–ºâ”‚ RESPOND  â”‚              â”‚
â”‚  â”‚         â”‚   â”‚         â”‚   â”‚          â”‚              â”‚
â”‚  â”‚ Alert   â”‚   â”‚ Assess  â”‚   â”‚ Diagnose â”‚              â”‚
â”‚  â”‚ fires   â”‚   â”‚ severityâ”‚   â”‚ & Fix    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                    â”‚                     â”‚
â”‚                                    â–¼                     â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                              â”‚ RECOVER  â”‚               â”‚
â”‚                              â”‚          â”‚               â”‚
â”‚                              â”‚ Verify   â”‚               â”‚
â”‚                              â”‚ & Stable â”‚               â”‚
â”‚                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                   â”‚                      â”‚
â”‚                                   â–¼                      â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                            â”‚  LEARN     â”‚               â”‚
â”‚                            â”‚            â”‚               â”‚
â”‚                            â”‚ Postmortem â”‚               â”‚
â”‚                            â”‚ & Improve  â”‚               â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â”‚  Key Metrics:                                           â”‚
â”‚  TTD (Time to Detect) + TTT (Time to Triage)           â”‚
â”‚  + TTR (Time to Resolve) = Total Incident Duration     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.2 Incident Severity Classification

| Severity | Impact | Examples | Response |
|----------|--------|----------|----------|
| SEV1 / P1 | Full outage, all users affected | Site down, data loss, security breach | All hands, war room, exec comms |
| SEV2 / P2 | Major degradation, many users | Payment failures, slow > 10x normal | On-call + team lead, customer comms |
| SEV3 / P3 | Minor degradation, some users | Feature partially broken, intermittent errors | On-call investigates, business hours |
| SEV4 / P4 | Minimal impact, cosmetic | UI glitch, non-critical feature | Ticket, fix in next sprint |

---

## 5.3 Incident Response Roles

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INCIDENT RESPONSE ROLES (ICS-inspired)                  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  INCIDENT COMMANDER (IC)         â”‚                   â”‚
â”‚  â”‚  â€¢ Owns the incident             â”‚                   â”‚
â”‚  â”‚  â€¢ Coordinates response          â”‚                   â”‚
â”‚  â”‚  â€¢ Makes decisions               â”‚                   â”‚
â”‚  â”‚  â€¢ Delegates tasks               â”‚                   â”‚
â”‚  â”‚  â€¢ NOT debugging (managing)      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚               â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â–¼            â–¼           â–¼             â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚Comms â”‚ â”‚ Tech   â”‚ â”‚Subject â”‚ â”‚Scribe    â”‚          â”‚
â”‚  â”‚Lead  â”‚ â”‚ Lead   â”‚ â”‚Matter  â”‚ â”‚          â”‚          â”‚
â”‚  â”‚      â”‚ â”‚        â”‚ â”‚Experts â”‚ â”‚          â”‚          â”‚
â”‚  â”‚Updateâ”‚ â”‚Debug & â”‚ â”‚        â”‚ â”‚Document  â”‚          â”‚
â”‚  â”‚statusâ”‚ â”‚fix the â”‚ â”‚Called  â”‚ â”‚timeline, â”‚          â”‚
â”‚  â”‚page, â”‚ â”‚issue   â”‚ â”‚in as   â”‚ â”‚actions,  â”‚          â”‚
â”‚  â”‚Slack,â”‚ â”‚        â”‚ â”‚needed  â”‚ â”‚decisions â”‚          â”‚
â”‚  â”‚execs â”‚ â”‚        â”‚ â”‚        â”‚ â”‚          â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.4 Incident Response Playbook

### Step-by-Step Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INCIDENT RESPONSE PLAYBOOK                              â”‚
â”‚                                                          â”‚
â”‚  1. ALERT RECEIVED                                      â”‚
â”‚     â–¡ Acknowledge alert within 5 minutes                â”‚
â”‚     â–¡ Read alert summary and annotations                â”‚
â”‚     â–¡ Open runbook link from alert                      â”‚
â”‚                                                          â”‚
â”‚  2. ASSESS SEVERITY                                     â”‚
â”‚     â–¡ Check: Is service completely down or degraded?    â”‚
â”‚     â–¡ Check: How many users affected?                   â”‚
â”‚     â–¡ Check: Is data being lost?                        â”‚
â”‚     â–¡ Assign SEV level (1-4)                            â”‚
â”‚                                                          â”‚
â”‚  3. COMMUNICATE                                         â”‚
â”‚     â–¡ Post in #incidents Slack channel                  â”‚
â”‚     â–¡ If SEV1/2: Start war room / video call            â”‚
â”‚     â–¡ If SEV1: Notify stakeholders                      â”‚
â”‚     â–¡ Update status page if customer-facing             â”‚
â”‚                                                          â”‚
â”‚  4. DIAGNOSE                                            â”‚
â”‚     â–¡ Check dashboards (Grafana)                        â”‚
â”‚     â–¡ Check recent deployments                          â”‚
â”‚     â–¡ Check logs (Loki/ELK)                             â”‚
â”‚     â–¡ Check traces for errors (Tempo/Jaeger)            â”‚
â”‚     â–¡ Check infrastructure (K8s, cloud)                 â”‚
â”‚                                                          â”‚
â”‚  5. MITIGATE                                            â”‚
â”‚     â–¡ Rollback if caused by deployment                  â”‚
â”‚     â–¡ Scale up if capacity issue                        â”‚
â”‚     â–¡ Failover if infrastructure issue                  â”‚
â”‚     â–¡ Toggle feature flag if feature is causing issue   â”‚
â”‚     â–¡ Apply hotfix if quick fix available               â”‚
â”‚                                                          â”‚
â”‚  6. VERIFY RECOVERY                                     â”‚
â”‚     â–¡ Confirm metrics return to normal                  â”‚
â”‚     â–¡ Check error rate drops                            â”‚
â”‚     â–¡ Verify customer-facing functionality              â”‚
â”‚     â–¡ Monitor for 15+ minutes for stability             â”‚
â”‚                                                          â”‚
â”‚  7. CLOSE INCIDENT                                      â”‚
â”‚     â–¡ Resolve alert in PagerDuty/OpsGenie               â”‚
â”‚     â–¡ Update status page to "Resolved"                  â”‚
â”‚     â–¡ Post summary in #incidents channel                â”‚
â”‚     â–¡ Schedule postmortem within 48 hours               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.5 Communication Templates

### Status Page Update

```
[Investigating] 14:05 UTC
We are investigating reports of elevated error rates
on the API. Some users may experience 500 errors.

[Identified] 14:15 UTC
The issue has been identified as a database connection
pool exhaustion following a traffic spike. We are
scaling the connection pool and database replicas.

[Monitoring] 14:35 UTC
A fix has been deployed. Error rates are returning to
normal. We are monitoring the situation.

[Resolved] 15:00 UTC
The issue has been fully resolved. API error rates
are back to normal levels. A postmortem will follow.
Total impact duration: 55 minutes.
```

### Slack Incident Channel Format

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  #incident-2024-01-15-api-errors                        â”‚
â”‚                                                          â”‚
â”‚  ğŸ“‹ INCIDENT HEADER                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  Severity: SEV2                                          â”‚
â”‚  IC: @alice                                              â”‚
â”‚  Tech Lead: @bob                                         â”‚
â”‚  Comms Lead: @carol                                      â”‚
â”‚  Start Time: 2024-01-15 14:05 UTC                       â”‚
â”‚  Status: Active                                          â”‚
â”‚  Impact: ~15% of API requests returning 500              â”‚
â”‚  Status Page: Updated                                    â”‚
â”‚                                                          â”‚
â”‚  ğŸ• TIMELINE                                            â”‚
â”‚  14:05 - Alert fired: APIHighErrorRate                  â”‚
â”‚  14:07 - @alice acknowledged, assessing                 â”‚
â”‚  14:10 - Declared SEV2, war room started                â”‚
â”‚  14:15 - Root cause: DB connection pool exhausted       â”‚
â”‚  14:20 - Scaling DB connections from 50 â†’ 200           â”‚
â”‚  14:30 - Error rate dropping                            â”‚
â”‚  14:45 - Error rate < 0.1%, monitoring                  â”‚
â”‚  15:00 - Incident resolved                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.6 Common Diagnostic Steps

| Check | Tools | What to Look For |
|-------|-------|------------------|
| Recent deployments | Git, CI/CD, ArgoCD | Deploy correlating with incident start |
| Error rate metrics | Grafana, Prometheus | Spike timing, affected services |
| Logs | Loki, ELK, kubectl logs | Error messages, stack traces |
| Traces | Tempo, Jaeger | Slow spans, error spans, root cause |
| Infrastructure | kubectl, cloud console | Pod restarts, node issues, OOM |
| Dependencies | Service map, status pages | External service outage |
| Config changes | Terraform, ConfigMaps | Recent config rollout |

---

## 5.7 Mitigation Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MITIGATION DECISION TREE                                â”‚
â”‚                                                          â”‚
â”‚  Was there a recent deployment?                         â”‚
â”‚  â”œâ”€ YES â†’ ROLLBACK deployment                          â”‚
â”‚  â””â”€ NO â”€â”€â–º Is it a capacity issue?                      â”‚
â”‚            â”œâ”€ YES â†’ SCALE UP (replicas, resources)      â”‚
â”‚            â””â”€ NO â”€â”€â–º Is a dependency down?               â”‚
â”‚                      â”œâ”€ YES â†’ FAILOVER or circuit break â”‚
â”‚                      â””â”€ NO â”€â”€â–º Is it a feature bug?      â”‚
â”‚                                â”œâ”€ YES â†’ FEATURE FLAG offâ”‚
â”‚                                â””â”€ NO â”€â”€â–º Is it infra?    â”‚
â”‚                                          â”œâ”€ YES â†’ REBOOTâ”‚
â”‚                                          â””â”€ NO â†’ HOTFIX â”‚
â”‚                                                          â”‚
â”‚  Priority: Restore service first, investigate later.    â”‚
â”‚  "Stop the bleeding, then do surgery."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Troubleshooting Tips

| Problem | Cause | Solution |
|---------|-------|----------|
| Slow response | No clear process | Document and drill incident playbook |
| Chaos in war room | No incident commander | Always assign IC first |
| Customers unaware | No status page updates | Assign comms lead role |
| Repeat incidents | No postmortem follow-up | Mandate postmortem + action items |
| Can't diagnose | Missing observability | Improve instrumentation post-incident |

---

## Quick Revision Questions

1. **What are the five phases of the incident lifecycle?**
2. **What are the key roles in incident response and what does each do?**
3. **How do you decide the severity of an incident?**
4. **What is the first priority during an incident: finding root cause or restoring service?**
5. **What communication should happen during a SEV1 incident?**
6. **What is a mitigation decision tree and how does it help during incidents?**

---

[â† Previous: On-Call Management](04-on-call-management.md) | [Back to README](../README.md) | [Next: Incident Management â†’](06-incident-management.md)
