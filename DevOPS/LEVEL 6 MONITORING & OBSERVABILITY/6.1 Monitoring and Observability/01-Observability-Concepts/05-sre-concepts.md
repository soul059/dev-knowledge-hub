# Chapter 5: SRE Concepts â€” SLI, SLO, SLA & Error Budgets

[â† Previous: Observability Maturity Model](04-observability-maturity-model.md) | [Back to README](../README.md) | [Next: Unit 2 â€” What Are Metrics? â†’](../02-Metrics/01-what-are-metrics.md)

---

## Overview

Site Reliability Engineering (SRE) provides a framework for managing reliability through **data-driven decisions**. The core concepts â€” **SLI, SLO, SLA, and Error Budgets** â€” form the backbone of how modern organizations balance reliability with feature velocity.

---

## 5.1 The SRE Reliability Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SLA                           â”‚
â”‚         (Legal/Business Agreement)               â”‚
â”‚    "We promise 99.9% uptime or we pay you"      â”‚
â”‚                                                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚    â”‚              SLO                  â”‚         â”‚
â”‚    â”‚     (Internal Target)             â”‚         â”‚
â”‚    â”‚  "We target 99.95% availability"  â”‚         â”‚
â”‚    â”‚                                   â”‚         â”‚
â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚         â”‚
â”‚    â”‚    â”‚        SLI          â”‚        â”‚         â”‚
â”‚    â”‚    â”‚   (Measurement)     â”‚        â”‚         â”‚
â”‚    â”‚    â”‚ "Currently 99.97%"  â”‚        â”‚         â”‚
â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚         â”‚
â”‚    â”‚                                   â”‚         â”‚
â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚         â”‚
â”‚    â”‚    â”‚   Error Budget      â”‚        â”‚         â”‚
â”‚    â”‚    â”‚  (Remaining room)   â”‚        â”‚         â”‚
â”‚    â”‚    â”‚ "0.05% left to use" â”‚        â”‚         â”‚
â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.2 SLI â€” Service Level Indicator

> **SLI** = A carefully defined quantitative measure of some aspect of the level of service provided.

### What Makes a Good SLI?

An SLI is a **ratio** expressed as:

```
SLI = (Good events / Total events) Ã— 100%

Example:
SLI = (Successful HTTP requests / Total HTTP requests) Ã— 100%
SLI = (Requests served < 300ms / Total requests) Ã— 100%
```

### Common SLI Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SLI CATEGORIES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Category       â”‚ SLI Definition                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Availability   â”‚ % of requests that succeed (non-5xx)    â”‚
â”‚                â”‚ Good: status != 5xx / Total requests    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency        â”‚ % of requests faster than threshold     â”‚
â”‚                â”‚ Good: latency < 300ms / Total requests  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Throughput     â”‚ % of time system handles expected load   â”‚
â”‚                â”‚ Good: rps > baseline / Total time        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Correctness    â”‚ % of requests returning correct results â”‚
â”‚                â”‚ Good: correct results / Total results    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Freshness      â”‚ % of data updated within threshold      â”‚
â”‚                â”‚ Good: data age < 1min / Total records    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Durability     â”‚ % of data retrievable when expected      â”‚
â”‚                â”‚ Good: data found / Total expected data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Measuring SLIs with Prometheus

```promql
# Availability SLI
sum(rate(http_requests_total{status!~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))

# Latency SLI (% requests under 300ms)
sum(rate(http_request_duration_seconds_bucket{le="0.3"}[5m]))
/
sum(rate(http_request_duration_seconds_count[5m]))

# Current SLI value over 30 days
avg_over_time(
  (
    sum(rate(http_requests_total{status!~"5.."}[5m]))
    /
    sum(rate(http_requests_total[5m]))
  )[30d:5m]
)
```

---

## 5.3 SLO â€” Service Level Objective

> **SLO** = A target value (or range) for a service level as measured by an SLI.

### SLO = SLI + Target + Window

```
SLO = "99.9% of requests succeed over a 30-day rolling window"
       â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Target                        Measurement Window

SLO = "95% of requests complete in under 300ms over 7 days"
```

### The Nines Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Uptime % â”‚ Downtime/Year â”‚ Downtime/Monthâ”‚ Downtime/Day     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 99%      â”‚ 3.65 days     â”‚ 7.31 hours    â”‚ 14.4 minutes     â”‚
â”‚ 99.9%    â”‚ 8.77 hours    â”‚ 43.83 min     â”‚ 1.44 minutes     â”‚
â”‚ 99.95%   â”‚ 4.38 hours    â”‚ 21.92 min     â”‚ 43.2 seconds     â”‚
â”‚ 99.99%   â”‚ 52.6 minutes  â”‚ 4.38 min      â”‚ 8.64 seconds     â”‚
â”‚ 99.999%  â”‚ 5.26 minutes  â”‚ 26.3 sec      â”‚ 864 milliseconds â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ Each additional "nine" is roughly 10x harder and more expensive!
```

### How to Choose an SLO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Choosing the Right SLO Target:                         â”‚
â”‚                                                          â”‚
â”‚  Too Low (e.g., 99%):                                   â”‚
â”‚  â€¢ Users have poor experience                           â”‚
â”‚  â€¢ Business loses revenue                               â”‚
â”‚  â€¢ Doesn't reflect user expectations                    â”‚
â”‚                                                          â”‚
â”‚  Too High (e.g., 99.999%):                             â”‚
â”‚  â€¢ Extremely expensive to achieve                       â”‚
â”‚  â€¢ Engineering velocity grinds to a halt               â”‚
â”‚  â€¢ False sense of "reliability theater"                 â”‚
â”‚                                                          â”‚
â”‚  Just Right:                                            â”‚
â”‚  â€¢ Slightly better than current performance             â”‚
â”‚  â€¢ Reflects actual user impact thresholds               â”‚
â”‚  â€¢ Allows room for innovation (error budget)            â”‚
â”‚                                                          â”‚
â”‚  Rule of thumb: SLO should be stricter than SLA         â”‚
â”‚  If SLA = 99.9%, set SLO = 99.95%                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.4 SLA â€” Service Level Agreement

> **SLA** = A contract with users that includes consequences (typically financial) for meeting or missing the SLO.

### SLA vs SLO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚   SLA (External Promise â€” has consequences)              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ "We guarantee 99.9% availability.          â”‚         â”‚
â”‚   â”‚  If we fail, we credit your account 10%."  â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                           â”‚
â”‚   SLO (Internal Target â€” no external consequence)        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ "We target 99.95% availability internally. â”‚         â”‚
â”‚   â”‚  This gives us buffer above the SLA."      â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                           â”‚
â”‚   SLI (Measurement)                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ "Our current availability is 99.97%."      â”‚         â”‚
â”‚   â”‚  We're meeting both SLO and SLA.           â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                           â”‚
â”‚   Relationship:                                          â”‚
â”‚   SLI â”€â”€measuresâ”€â”€â–º SLO â”€â”€informsâ”€â”€â–º SLA                â”‚
â”‚                                                           â”‚
â”‚   Typically:                                             â”‚
â”‚   SLA < SLO < Actual Performance                        â”‚
â”‚   99.9%  < 99.95% < 99.97%                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common SLA Structures

| Provider | Uptime SLA | Credit |
|----------|-----------|--------|
| AWS EC2 | 99.99% | 10-30% credit for violations |
| Google Cloud | 99.95% | 10-50% credit |
| Azure | 99.95-99.99% | 10-100% credit |
| Stripe API | 99.99% | Not financial (SLO published) |

---

## 5.5 Error Budgets

> **Error Budget** = The maximum amount of unreliability you can tolerate while still meeting your SLO.

### Calculating Error Budget

```
Error Budget = 1 - SLO Target

If SLO = 99.9% availability:
  Error Budget = 1 - 0.999 = 0.1%

Over 30 days:
  Error Budget = 30 days Ã— 24 hours Ã— 60 minutes Ã— 0.001
               = 43.2 minutes of downtime allowed

Over 30 days with 1M requests:
  Error Budget = 1,000,000 Ã— 0.001 = 1,000 failed requests allowed
```

### Error Budget Visualization

```
SLO Target: 99.9% over 30 days
Error Budget: 43.2 minutes

Day 1                                              Day 30
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Error Budget Remaining:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
100%                                    Consumed â”€â”€â–º  0%

Day  5: 5min incident    â”€â”€â”€ Budget: 38.2 min remaining (88%)
Day 12: 2min blip        â”€â”€â”€ Budget: 36.2 min remaining (84%)
Day 18: 15min outage     â”€â”€â”€ Budget: 21.2 min remaining (49%)
Day 22: Deploy freeze!   â”€â”€â”€ Budget getting low
Day 25: 10min incident   â”€â”€â”€ Budget: 11.2 min remaining (26%)
Day 28: âš ï¸ BUDGET CRITICAL â”€â”€â”€ Stop risky changes
```

### Error Budget Policy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ERROR BUDGET POLICY                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Budget > 75%     â”‚ âœ… Normal operations                 â”‚
â”‚                  â”‚ â€¢ Deploy freely                      â”‚
â”‚                  â”‚ â€¢ Run experiments                    â”‚
â”‚                  â”‚ â€¢ Try new features                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Budget 50-75%    â”‚ âš ï¸ Caution                          â”‚
â”‚                  â”‚ â€¢ Continue deploying but carefully   â”‚
â”‚                  â”‚ â€¢ Prioritize reliability work        â”‚
â”‚                  â”‚ â€¢ Review recent incidents            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Budget 25-50%    â”‚ ğŸŸ¡ Reduced velocity                  â”‚
â”‚                  â”‚ â€¢ Only well-tested changes           â”‚
â”‚                  â”‚ â€¢ Focus on reliability improvements  â”‚
â”‚                  â”‚ â€¢ Increase testing coverage          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Budget < 25%     â”‚ ğŸ”´ Freeze                            â”‚
â”‚                  â”‚ â€¢ No new features deployed           â”‚
â”‚                  â”‚ â€¢ Only reliability improvements      â”‚
â”‚                  â”‚ â€¢ Postmortem all recent incidents     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Budget = 0%      â”‚ ğŸš« Full stop                         â”‚
â”‚                  â”‚ â€¢ Emergency reliability work only    â”‚
â”‚                  â”‚ â€¢ Escalate to leadership             â”‚
â”‚                  â”‚ â€¢ SLO review needed                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.6 SLO-Based Alerting

Traditional alerting uses static thresholds. SLO-based alerting uses **burn rate**:

```
Burn Rate = rate of error budget consumption

If burn rate = 1:  Budget consumed exactly on schedule (just meets SLO)
If burn rate = 2:  Budget consumed 2x faster (will exhaust in 15 days)
If burn rate = 10: Budget consumed 10x faster (will exhaust in 3 days)
If burn rate = 36: Budget consumed 36x faster (emergency!)
```

### Multi-Window, Multi-Burn-Rate Alerting

```yaml
# Prometheus alerting rules for SLO burn rate
groups:
  - name: slo-alerts
    rules:
      # Critical: high burn rate over short window
      - alert: HighErrorBudgetBurn
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[1h]))
            / sum(rate(http_requests_total[1h]))
          ) > (14.4 * 0.001)
          and
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            / sum(rate(http_requests_total[5m]))
          ) > (14.4 * 0.001)
        labels:
          severity: critical
        annotations:
          summary: "High error budget burn rate (>14.4x)"
          
      # Warning: moderate burn rate over longer window
      - alert: ErrorBudgetBurnWarning
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[6h]))
            / sum(rate(http_requests_total[6h]))
          ) > (6 * 0.001)
          and
          (
            sum(rate(http_requests_total{status=~"5.."}[30m]))
            / sum(rate(http_requests_total[30m]))
          ) > (6 * 0.001)
        labels:
          severity: warning
        annotations:
          summary: "Elevated error budget burn rate (>6x)"
```

---

## 5.7 Putting It All Together

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SRE RELIABILITY WORKFLOW                         â”‚
â”‚                                                               â”‚
â”‚  1. Define SLIs                                              â”‚
â”‚     "What matters to our users?"                             â”‚
â”‚     â”œâ”€â”€ Availability: Can they use the service?              â”‚
â”‚     â”œâ”€â”€ Latency: Is it fast enough?                          â”‚
â”‚     â””â”€â”€ Correctness: Are results accurate?                   â”‚
â”‚                                                               â”‚
â”‚  2. Set SLOs                                                  â”‚
â”‚     "What target should we aim for?"                         â”‚
â”‚     â”œâ”€â”€ 99.9% availability over 30 days                     â”‚
â”‚     â””â”€â”€ 95% of requests < 300ms over 30 days                â”‚
â”‚                                                               â”‚
â”‚  3. Calculate Error Budget                                   â”‚
â”‚     "How much room do we have?"                              â”‚
â”‚     â””â”€â”€ 43.2 minutes of downtime per month                  â”‚
â”‚                                                               â”‚
â”‚  4. Set Up Alerting                                          â”‚
â”‚     "When should we act?"                                    â”‚
â”‚     â”œâ”€â”€ Burn rate > 14x â†’ Page on-call (critical)           â”‚
â”‚     â””â”€â”€ Burn rate > 6x  â†’ Create ticket (warning)           â”‚
â”‚                                                               â”‚
â”‚  5. Apply Error Budget Policy                                â”‚
â”‚     "What changes based on budget remaining?"                â”‚
â”‚     â”œâ”€â”€ > 50%: Ship features freely                         â”‚
â”‚     â””â”€â”€ < 25%: Freeze features, fix reliability             â”‚
â”‚                                                               â”‚
â”‚  6. Review & Iterate                                         â”‚
â”‚     "Were our SLOs right?"                                   â”‚
â”‚     â”œâ”€â”€ Monthly SLO review meeting                          â”‚
â”‚     â”œâ”€â”€ Adjust targets based on user feedback               â”‚
â”‚     â””â”€â”€ Postmortem for every SLO violation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.8 Real-World Scenario

ğŸŒ **Scenario: Managing Error Budget for a Payment Service**

```
Service: payment-api
SLO: 99.95% availability over 30 days
Error Budget: 21.6 minutes/month

Week 1:
â”œâ”€â”€ Deployed new payment gateway integration
â”œâ”€â”€ 3-minute outage during rollout (bad config)
â”œâ”€â”€ Budget remaining: 18.6 min (86%)
â””â”€â”€ Action: Postmortem, add config validation

Week 2:
â”œâ”€â”€ Normal operations
â”œâ”€â”€ 0 downtime
â”œâ”€â”€ Budget remaining: 18.6 min (86%)
â””â”€â”€ Action: Ship new features, run experiments

Week 3:
â”œâ”€â”€ Database failover took 8 minutes
â”œâ”€â”€ Budget remaining: 10.6 min (49%)
â”œâ”€â”€ Entered "caution" zone
â””â”€â”€ Action: Prioritize DB failover automation

Week 4:
â”œâ”€â”€ Another 7 minutes of downtime
â”œâ”€â”€ Budget remaining: 3.6 min (17%)
â”œâ”€â”€ BUDGET CRITICAL!
â””â”€â”€ Action: Feature freeze, all hands on reliability

Month-end Review:
â”œâ”€â”€ SLO met (barely): actual = 99.96%
â”œâ”€â”€ But used 83% of budget
â”œâ”€â”€ Decision: Invest in automated DB failover
â””â”€â”€ Next month target: Reduce incidents by 50%
```

---

## 5.9 Troubleshooting Tips

| Problem | Cause | Solution |
|---------|-------|----------|
| SLO always violated | Target too aggressive | Lower SLO to achievable level, then improve |
| Error budget never used | SLO too lenient | Tighten SLO to push innovation |
| Team ignores error budget | No policy enforcement | Get management buy-in for error budget policy |
| SLI doesn't reflect user pain | Wrong metric chosen | Measure at user-facing boundary, not internal |
| Burn rate alerts too noisy | Window too short | Use multi-window alerting (5m AND 1h) |

---

## Summary Table

| Concept | Definition | Example |
|---------|-----------|---------|
| **SLI** | Quantitative measure of service performance | 99.97% of requests succeed |
| **SLO** | Target value for an SLI over a time window | 99.9% availability over 30 days |
| **SLA** | Contract with consequences for missing SLO | 99.9% uptime or 10% credit |
| **Error Budget** | Maximum allowed unreliability (1 - SLO) | 43.2 min downtime/month |
| **Burn Rate** | Speed of error budget consumption | 2x = budget gone in 15 days |
| **Error Budget Policy** | Actions based on remaining budget | <25% â†’ feature freeze |

---

## Quick Revision Questions

1. **Define SLI, SLO, and SLA. How do they relate to each other? Draw the hierarchy.**

2. **If your SLO is 99.95% availability over a 30-day window, calculate the error budget in minutes.**

3. **What is a "burn rate" in the context of SLO-based alerting? Why is it superior to simple threshold-based alerts?**

4. **Describe the error budget policy. What actions should a team take when the error budget drops below 25%?**

5. **Why should the SLO always be stricter than the SLA? What problems arise if they're set equal?**

6. **Give an example of how error budgets help balance reliability work with feature development.**

---

[â† Previous: Observability Maturity Model](04-observability-maturity-model.md) | [Back to README](../README.md) | [Next: Unit 2 â€” What Are Metrics? â†’](../02-Metrics/01-what-are-metrics.md)
