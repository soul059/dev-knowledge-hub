# Unit 12: Special Sorting Problems â€” External Sorting

[â† Previous: K-Way Merge](05-k-way-merge.md) | [Back to README â†’](../README.md)

---

## Overview

**External Sorting** handles datasets too large to fit in RAM. When you have 100 GB of data but only 1 GB of RAM, you need to sort using disk I/O efficiently. The classic solution is **External Merge Sort**.

---

## The Problem

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Data: 100 GB on disk                                      â”‚
  â”‚  RAM:  1 GB available                                       â”‚
  â”‚                                                              â”‚
  â”‚  Can't load everything into memory!                        â”‚
  â”‚  Disk reads/writes are ~100,000x slower than RAM access    â”‚
  â”‚                                                              â”‚
  â”‚  Goal: Sort the data while minimizing disk I/O             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Key insight: Disk access is sequential-friendly.
  Reading 1 MB sequentially â‰ˆ reading 1 byte randomly.
  â†’ Design for SEQUENTIAL access, minimize RANDOM seeks.
```

---

## External Merge Sort: Two Phases

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  PHASE 1: CREATE SORTED RUNS                               â”‚
  â”‚                                                             â”‚
  â”‚  Read M elements at a time (M fits in RAM)                 â”‚
  â”‚  Sort them in memory                                        â”‚
  â”‚  Write sorted "run" back to disk                           â”‚
  â”‚                                                             â”‚
  â”‚  Result: N/M sorted runs on disk                           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  PHASE 2: MERGE RUNS                                       â”‚
  â”‚                                                             â”‚
  â”‚  Use K-way merge to combine sorted runs                    â”‚
  â”‚  K = number of runs that fit in memory simultaneously     â”‚
  â”‚  (K = M / buffer_size - 1, reserve 1 buffer for output)   â”‚
  â”‚                                                             â”‚
  â”‚  If more runs than K: multi-pass merge                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Visual Walkthrough

```
  DATA: 100 GB on disk,  RAM: 1 GB
  
  PHASE 1 â€” Create Sorted Runs:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Read 1 GB chunk â†’ Sort in RAM â†’ Write to disk    â”‚
  â”‚  Repeat 100 times                                   â”‚
  â”‚                                                     â”‚
  â”‚  Result: 100 sorted runs, each 1 GB                â”‚
  â”‚                                                     â”‚
  â”‚  Run 0: [sorted 1 GB]                              â”‚
  â”‚  Run 1: [sorted 1 GB]                              â”‚
  â”‚  ...                                                â”‚
  â”‚  Run 99: [sorted 1 GB]                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  PHASE 2 â€” K-Way Merge:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  RAM: split into K input buffers + 1 output buffer â”‚
  â”‚                                                     â”‚
  â”‚  With 1 GB RAM, 10 MB buffers:                     â”‚
  â”‚  K = 99 input buffers + 1 output buffer            â”‚
  â”‚  = 100 Ã— 10 MB = 1 GB âœ“                           â”‚
  â”‚                                                     â”‚
  â”‚  Read 10 MB from each of 99 runs into buffers     â”‚
  â”‚  K-way merge using min-heap                        â”‚
  â”‚  When output buffer full â†’ write to disk           â”‚
  â”‚  When input buffer empty â†’ read next 10 MB block   â”‚
  â”‚                                                     â”‚
  â”‚  Since K=99 â‰¥ 100... wait, need 100 runs!         â”‚
  â”‚  Two passes needed if K < number of runs           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Pass Merge

```
  If 100 runs but K = 10 (can merge 10 at a time):
  
  Pass 1: Merge groups of 10 runs:
  Runs 0-9   â†’ Merged Run A  (10 GB)
  Runs 10-19 â†’ Merged Run B  (10 GB)
  ...
  Runs 90-99 â†’ Merged Run J  (10 GB)
  â†’ 10 merged runs
  
  Pass 2: Merge all 10 merged runs:
  Runs A-J â†’ Final sorted output (100 GB)
  
  Total passes: âŒˆlog_K(R)âŒ‰ where R = initial number of runs
  With K=10, R=100: âŒˆlogâ‚â‚€(100)âŒ‰ = 2 passes
```

---

## Pseudocode

```
  EXTERNAL_MERGE_SORT(file, memory_size):
    
    // Phase 1: Create sorted runs
    runs = []
    while data remaining in file:
        chunk = read(file, memory_size)
        sort(chunk)                    // any in-memory sort
        run = write_to_temp_file(chunk)
        runs.append(run)
    
    // Phase 2: K-way merge
    K = memory_size / buffer_size - 1
    
    while len(runs) > 1:
        new_runs = []
        for i in range(0, len(runs), K):
            group = runs[i : i+K]
            merged = k_way_merge(group, buffer_size)
            new_runs.append(merged)
        runs = new_runs
    
    return runs[0]  // final sorted file
```

---

## Java Implementation (Simplified)

```java
import java.io.*;
import java.util.*;

public class ExternalSort {
    
    static final int CHUNK_SIZE = 1_000_000; // elements per run
    
    public static void externalSort(String inputFile, String outputFile) 
            throws IOException {
        
        // Phase 1: Create sorted runs
        List<String> runFiles = createSortedRuns(inputFile);
        
        // Phase 2: K-way merge
        mergeRuns(runFiles, outputFile);
        
        // Cleanup temp files
        for (String f : runFiles) new File(f).delete();
    }
    
    static List<String> createSortedRuns(String inputFile) 
            throws IOException {
        List<String> runFiles = new ArrayList<>();
        BufferedReader reader = new BufferedReader(new FileReader(inputFile));
        
        int runId = 0;
        while (true) {
            List<Integer> chunk = new ArrayList<>();
            String line;
            for (int i = 0; i < CHUNK_SIZE; i++) {
                line = reader.readLine();
                if (line == null) break;
                chunk.add(Integer.parseInt(line.trim()));
            }
            if (chunk.isEmpty()) break;
            
            Collections.sort(chunk);  // Sort in memory
            
            String runFile = "run_" + runId + ".tmp";
            BufferedWriter writer = new BufferedWriter(new FileWriter(runFile));
            for (int val : chunk) {
                writer.write(val + "\n");
            }
            writer.close();
            
            runFiles.add(runFile);
            runId++;
        }
        reader.close();
        return runFiles;
    }
    
    static void mergeRuns(List<String> runFiles, String outputFile) 
            throws IOException {
        // Min-heap: (value, readerIndex)
        PriorityQueue<long[]> heap = new PriorityQueue<>(
            (a, b) -> Long.compare(a[0], b[0])
        );
        
        BufferedReader[] readers = new BufferedReader[runFiles.size()];
        for (int i = 0; i < runFiles.size(); i++) {
            readers[i] = new BufferedReader(new FileReader(runFiles.get(i)));
            String line = readers[i].readLine();
            if (line != null) {
                heap.offer(new long[]{Long.parseLong(line.trim()), i});
            }
        }
        
        BufferedWriter writer = new BufferedWriter(new FileWriter(outputFile));
        
        while (!heap.isEmpty()) {
            long[] min = heap.poll();
            writer.write(min[0] + "\n");
            
            int idx = (int) min[1];
            String line = readers[idx].readLine();
            if (line != null) {
                heap.offer(new long[]{Long.parseLong(line.trim()), idx});
            }
        }
        
        writer.close();
        for (BufferedReader r : readers) r.close();
    }
}
```

---

## Replacement Selection (Longer Runs)

```
  Standard approach: runs = RAM size M
  
  Replacement Selection: runs â‰ˆ 2M on average!
  
  Algorithm:
  1. Fill RAM with M elements (use a min-heap)
  2. Output minimum to current run
  3. Read next element from input
     - If new element â‰¥ last output â†’ add to heap
     - If new element < last output â†’ mark for NEXT run
  4. When all heap elements are marked â†’ start new run
  
  Expected run length: 2M (proven by Knuth)
  â†’ Fewer runs â†’ fewer merge passes â†’ faster!
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  100 GB data, 1 GB RAM:                                   â”‚
  â”‚  Standard: 100 runs of 1 GB                               â”‚
  â”‚  Replacement Selection: ~50 runs of ~2 GB                 â”‚
  â”‚  â†’ May save one entire merge pass                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complexity Analysis

```
  N = total data size
  M = memory size
  B = disk block size
  
  Phase 1 (create runs):
    Read entire file: N/B disk reads
    Write sorted runs: N/B disk writes
    In-memory sort: N/M runs Ã— O(M log M) each = O(N log M)
    Disk I/O: 2N/B
  
  Phase 2 (merge):
    Passes: âŒˆlog_K(N/M)âŒ‰  where K â‰ˆ M/B
    Each pass: read + write entire data = 2N/B
    Total I/O: 2N/B Ã— âŒˆlog_K(N/M)âŒ‰
  
  Total disk I/O: 2N/B Ã— (1 + âŒˆlog_{M/B}(N/M)âŒ‰)
  
  Example: N = 100 GB, M = 1 GB, B = 4 KB
    K = M/B = 1 GB / 4 KB = 250,000
    Runs = N/M = 100
    Passes = âŒˆlogâ‚‚â‚…â‚€â‚€â‚€â‚€(100)âŒ‰ = 1 pass!
    
    Total I/O: 2 Ã— 100/0.004 Ã— 2 â‰ˆ 100,000 block reads/writes
    With large K, most datasets need just 1-2 merge passes.
```

---

## Optimizations

```
  1. Double Buffering: 
     Read next block while processing current â†’ hide I/O latency
  
  2. Replacement Selection:
     Produces ~2x longer runs â†’ fewer passes
  
  3. Large Block Size:
     Sequential I/O is fast â†’ use large buffers (1-64 MB)
  
  4. SSD Optimization:
     SSDs have lower seek penalty â†’ can use more random I/O
     But sequential is still faster
  
  5. Compression:
     Compress runs on disk â†’ less I/O, more CPU
     Worth it when I/O-bound (usually is)
  
  6. Parallel I/O:
     Read from multiple disks simultaneously
     Distribute runs across drives
```

---

## Summary Table

| Aspect | Details |
|--------|---------|
| When needed | Data > RAM |
| Algorithm | External Merge Sort |
| Phase 1 | Create sorted runs in memory |
| Phase 2 | K-way merge of runs |
| Key metric | Disk I/O operations |
| Merge passes | âŒˆlog_K(runs)âŒ‰ |
| Optimization | Replacement selection (2x longer runs) |
| Used in | Databases, OS virtual memory, big data |

---

## Quick Revision Questions

1. **Why is merge sort (not quick sort) used for external sorting?**
2. **Calculate the number of merge passes for 1 TB data, 4 GB RAM, and 4 MB buffers.**
3. **What is replacement selection and how does it produce longer runs?**
4. **Why is sequential disk access so much faster than random access?**
5. **How does the K-way merge from the previous chapter apply here?**
6. **What is double buffering and why does it help external sorting?**

---

## Course Complete!

Congratulations on completing all 12 units of Sorting Algorithms! ğŸ‰

Key takeaways:
- **Simple sorts** (Bubble, Selection, Insertion) â€” O(nÂ²) but useful for small n
- **Efficient comparison sorts** (Merge, Quick, Heap) â€” O(n log n) barrier
- **Non-comparison sorts** (Counting, Radix, Bucket) â€” O(n) with constraints
- **Choosing wisely** â€” depends on data, constraints, and requirements
- **Special problems** â€” DNF, frequency sort, linked lists, K-way merge, external sort

---

[â† Previous: K-Way Merge](05-k-way-merge.md) | [Back to README â†’](../README.md)
