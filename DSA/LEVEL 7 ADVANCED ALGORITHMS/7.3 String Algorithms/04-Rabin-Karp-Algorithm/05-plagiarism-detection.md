# Chapter 4.5 ‚Äî Plagiarism Detection with Rabin-Karp

> **Unit 4: Rabin-Karp Algorithm** | [Course Home](../README.md)

---

## üìã Chapter Overview

Plagiarism detection is one of the most practical applications of Rabin-Karp.
By hashing fixed-length chunks and comparing them across documents, we can
efficiently find copied content.

---

## 1. The Plagiarism Problem

```
  Given: Document A (source), Document B (suspect)
  Find:  Common substrings of length ‚â• k

  Example:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Doc A: "The quick brown fox jumps over"    ‚îÇ
  ‚îÇ  Doc B: "A slow brown fox jumps near"       ‚îÇ
  ‚îÇ                                              ‚îÇ
  ‚îÇ  Common substring: "brown fox jumps"         ‚îÇ
  ‚îÇ  Length: 15  ‚â•  k = 5  ‚Üí PLAGIARISM FLAG    ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Approach: k-gram Fingerprinting

```
  Step 1: Break both documents into k-grams (substrings of length k)
  Step 2: Hash each k-gram using rolling hash
  Step 3: Compare hash sets to find common fingerprints
  Step 4: Verify matches

  Document A: "abcdefgh"   k = 4
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  k-grams: "abcd" "bcde" "cdef" "defg" "efgh"
  hashes:    h‚ÇÅ     h‚ÇÇ     h‚ÇÉ     h‚ÇÑ     h‚ÇÖ

  Document B: "xyzdefgi"   k = 4
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  k-grams: "xyzd" "yzde" "zdef" "defg" "efgi"
  hashes:    h‚ÇÜ     h‚Çá     h‚Çà     h‚ÇÑ     h‚Çâ

  Common: h‚ÇÑ ‚Üí "defg" appears in both! ‚Üí MATCH
```

---

## 3. Architecture

```
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Document ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Tokenizer   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ   K-gram     ‚îÇ
  ‚îÇ    A     ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ  Generator   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                        ‚îÇ Rolling Hash ‚îÇ
                                        ‚îÇ  (Rabin-Karp)‚îÇ
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                        ‚îÇ  Hash Set A  ‚îÇ
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ Compare
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Document ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Same steps  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Hash Set B  ‚îÇ
  ‚îÇ    B     ‚îÇ     ‚îÇ              ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                        ‚îÇ Intersection ‚îÇ
                                        ‚îÇ  = Matches   ‚îÇ
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 4. Winnowing: Selecting Representative Fingerprints

```
  Problem: Storing ALL k-gram hashes is expensive for large documents.
  Solution: WINNOWING ‚Äî select only some hashes, deterministically.

  Winnowing Algorithm:
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Compute hashes for all k-grams: h‚ÇÅ, h‚ÇÇ, h‚ÇÉ, ...
  2. Use a sliding WINDOW of size w
  3. In each window, select the MINIMUM hash
  4. Record selected hashes as fingerprints

  Hashes:  [12] [45] [7] [23] [56] [3] [89] [34]
  Window 1: [12  45   7]  ‚Üí min = 7  ‚úì
  Window 2: [45   7  23]  ‚Üí min = 7  (already selected)
  Window 3: [ 7  23  56]  ‚Üí min = 7  (already selected)
  Window 4: [23  56   3]  ‚Üí min = 3  ‚úì
  Window 5: [56   3  89]  ‚Üí min = 3  (already selected)
  Window 6: [ 3  89  34]  ‚Üí min = 3  (already selected)

  Fingerprints: {7, 3}   (much smaller than full set!)

  Guarantee: Any common substring of length ‚â• k + w - 1
             will be detected.
```

---

## 5. Similarity Score

```
  After finding common fingerprints:

  Jaccard Similarity = |F_A ‚à© F_B| / |F_A ‚à™ F_B|

  Example:
    Fingerprints of A: {3, 7, 12, 23, 56}
    Fingerprints of B: {3, 7, 34, 45, 89}
    
    Intersection: {3, 7}       ‚Üí |‚à©| = 2
    Union: {3, 7, 12, 23, 34, 45, 56, 89}  ‚Üí |‚à™| = 8

    Similarity = 2/8 = 25%

  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Similarity  ‚îÇ Interpretation           ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ  0 - 10%    ‚îÇ Likely original          ‚îÇ
  ‚îÇ 10 - 30%    ‚îÇ Some overlap (may be ok) ‚îÇ
  ‚îÇ 30 - 60%    ‚îÇ Suspicious               ‚îÇ
  ‚îÇ 60 - 100%   ‚îÇ Highly likely plagiarism ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 6. Complete Algorithm

```
function DetectPlagiarism(docA, docB, k, threshold):
    // Step 1: Generate k-grams and hash
    hashesA = RollingHash(docA, k)    // set of all k-gram hashes
    hashesB = RollingHash(docB, k)

    // Step 2: Find common hashes
    common = hashesA ‚à© hashesB

    // Step 3: Compute similarity
    similarity = |common| / |hashesA ‚à™ hashesB|

    // Step 4: Report
    if similarity > threshold:
        flag as plagiarism
        for each h in common:
            report matching positions

    return similarity
```

---

## 7. Implementation ‚Äî Python

```python
def get_kgram_hashes(text, k, d=31, q=10**9 + 7):
    """Get all k-gram hashes with their positions using rolling hash."""
    n = len(text)
    if n < k:
        return {}
    
    # Precompute d^(k-1) mod q
    dk = 1
    for _ in range(k - 1):
        dk = (dk * d) % q
    
    # Hash first window
    h = 0
    for i in range(k):
        h = (h * d + ord(text[i])) % q
    
    hashes = {h: [0]}  # hash ‚Üí list of positions
    
    # Roll
    for i in range(1, n - k + 1):
        h = (d * (h - ord(text[i - 1]) * dk) + ord(text[i + k - 1])) % q
        if h in hashes:
            hashes[h].append(i)
        else:
            hashes[h] = [i]
    
    return hashes


def detect_plagiarism(doc_a, doc_b, k=5):
    """Detect plagiarism between two documents."""
    hashes_a = get_kgram_hashes(doc_a, k)
    hashes_b = get_kgram_hashes(doc_b, k)
    
    set_a = set(hashes_a.keys())
    set_b = set(hashes_b.keys())
    
    common = set_a & set_b
    union = set_a | set_b
    
    similarity = len(common) / len(union) if union else 0
    
    # Find actual matching substrings
    matches = []
    for h in common:
        for pos_a in hashes_a[h]:
            substr_a = doc_a[pos_a:pos_a + k]
            for pos_b in hashes_b[h]:
                substr_b = doc_b[pos_b:pos_b + k]
                if substr_a == substr_b:  # verify (avoid spurious hits)
                    matches.append((substr_a, pos_a, pos_b))
    
    return similarity, matches


# Example
doc_a = "The quick brown fox jumps over the lazy dog"
doc_b = "A lazy brown fox jumps over the fence"

sim, matches = detect_plagiarism(doc_a, doc_b, k=5)
print(f"Similarity: {sim:.1%}")
for text, pa, pb in matches[:10]:
    print(f"  '{text}' at A[{pa}] and B[{pb}]")
```

---

## 8. Scaling to Many Documents

```
  For N documents (e.g., student submissions):

  Naive: Compare every pair ‚Üí N(N-1)/2 comparisons

  Optimized with Rabin-Karp:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  1. Hash all k-grams in each document           ‚îÇ
  ‚îÇ  2. Build inverted index: hash ‚Üí {doc IDs}      ‚îÇ
  ‚îÇ  3. For each hash appearing in ‚â• 2 docs:        ‚îÇ
  ‚îÇ     Flag those document pairs                    ‚îÇ
  ‚îÇ  4. Only do detailed comparison on flagged pairs ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Inverted Index:
    hash_42  ‚Üí  {doc1, doc5, doc12}     ‚Üê docs 1,5,12 share a k-gram
    hash_73  ‚Üí  {doc3}                  ‚Üê unique
    hash_99  ‚Üí  {doc2, doc5}            ‚Üê docs 2,5 share a k-gram

  This is the foundation of tools like MOSS and Turnitin.
```

---

## üìù Summary Table

| Concept | Details |
|---------|---------|
| k-gram | Substring of length k |
| Fingerprinting | Hashing k-grams with rolling hash |
| Winnowing | Selecting representative min-hashes from windows |
| Similarity | Jaccard = |A ‚à© B| / |A ‚à™ B| |
| Time complexity | O(n‚ÇÅ + n‚ÇÇ) for two documents |
| Scaling | Inverted index for many documents |
| Real tools | MOSS, Turnitin, Copyscape |

---

## ‚ùì Quick Revision Questions

1. **What is a k-gram in the context of plagiarism detection?**
   <details><summary>Answer</summary>A contiguous substring of length k extracted from the document. All k-grams form a sliding window over the text.</details>

2. **Why use rolling hash instead of hashing each k-gram independently?**
   <details><summary>Answer</summary>Rolling hash computes each subsequent k-gram hash in O(1) by reusing the previous hash, giving O(n) total instead of O(nk) with independent hashing.</details>

3. **What does winnowing achieve?**
   <details><summary>Answer</summary>It reduces the number of stored fingerprints by selecting only the minimum hash in each sliding window, while guaranteeing that any sufficiently long common substring will still be detected.</details>

4. **How is Jaccard similarity computed?**
   <details><summary>Answer</summary>Jaccard = |intersection of fingerprint sets| / |union of fingerprint sets|. A value closer to 1 indicates higher similarity.</details>

5. **How does an inverted index help with N documents?**
   <details><summary>Answer</summary>It maps each fingerprint hash to the set of documents containing it. Documents sharing fingerprints can be quickly identified without pairwise comparison, reducing work from O(N¬≤) to near-linear.</details>

---

| [‚¨ÖÔ∏è Previous: Multiple Pattern Matching](04-multiple-pattern-matching.md) | [Next: Implementation ‚û°Ô∏è](06-implementation.md) |
|:---|---:|
